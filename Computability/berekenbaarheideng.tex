\chapter{Turing machines}\label{berekenbaarheid}

%===============================================================================
\section{The Turing Machine as a Decider or Recognizer}\label{turingmachines}

In the previous sections, we looked at two classes from the Chomsky
hierarchy. In particular we studied the machines that decide the
membership problem for these languages: whether a given string belongs to
a given language. We already understand that some (many?) languages
are beyond the decision power of DFAs, PDAs, and even LBAs. So it is
time to start studying machines that can do more. We use Turing
machines.

Roughly speaking, a TM has a tape infinite on two ends (unlimited would be enough)
consisting of squares (or cells) with one symbol each, a control
unit, a tape head that can read the symbol in a square, and write a
new one. The action of the machine depends on the internal state of
its control unit and the symbol being read. Possible actions are:
writing a symbol and moving the tape head. Figure~\ref{turing1} shows
the parts of the machine and the actions that are described by a
function $\delta$.

\begin{figure}[h]
	\centering
	\includegraphics[%
	  width=0.6\linewidth,
	  keepaspectratio]{turing1eng}
	\caption{Schema of a Turing machine\label{turing1}}
\end{figure}

\begin{wrapfigure}[7]{r}{.2\textwidth}
\includegraphics[width=.2\textwidth,keepaspectratio]{static/turing}
\end{wrapfigure}

 More formally:

\begin{definition}[Turing machine]
A {\bf Turing machine} is a 7-tuple
%
$(Q, \Sigma, \Gamma, \delta, q_s, q_a, q_r)$
%
with $Q, \Sigma, \Gamma$ finite sets:

\begin{itemize}
\item
$Q$ is the set of states

\item
$\Sigma$ is an input alphabet not containing \#

\item
$\Gamma$ is the tape alphabet; $\# \in \Gamma$ and $\Sigma \subset \Gamma$

\item
$q_s$ is the start state

\item
$q_a$ is the accepting end state


\item
$q_r$ is the rejecting end state and different from $q_a$


\item
$\delta$ is the transition function: it is a total function with
  signature

$~~~~~~~~~~Q \times \Gamma \rightarrow Q \times \Gamma \times \{L,R,S\}$

where $L, R, S$ denotes the direction the tape head moves after the transition (left, right, stay)

\end{itemize}
\end{definition}

The machine is initialized as follows:
\begin{itemize}
\item the symbols of a string from $\Sigma^*$ are put in subsequent
tape cells: they constitute the input string; all other cells contain
\#
\item the machine is put in start state $q_s$
\item the tape head is positioned at the leftmost symbol of the input
  string; if the input string is empty, it can be positioned anywhere
\end{itemize}

The machine works as follows: on the basis of the state of the
machine and the symbol under its head, $\delta$ determines the next
state of the machine, which symbol needs to be written, and the
direction of the head movement. This is repeated until
\begin{itemize}
\item the machine arrives in the state $q_a$: the input string is
  accepted and sometimes, the contents of the tape at this moment are
  considered the result of the computation
\item the machine arrives in the state $q_r$: the input string is
 rejected
\item neither of the above, and it continues ... the machine {\em
  loops} or is in an infinite loop
\end{itemize}

As with DFA and PDAs, there are alternative definitions of TMs, or
variants. Here are a few that do not seem to matter: they define {\em
  equivalent} machines. You can prove that, but you must start by
defining precisely what you mean by equivalent.

\begin{itemize}
\item the tape head can not stay at the same place: it must at every step move left of right
\item there can be more than one accepting or rejecting state
\item the tape is unlimited in only one direction (it is half-infinite)
\item there is more than one tape
\item the input alphabet has only two symbols\label{twosymbols}
\item there are only three states
\item the machine has two stacks instead of a tape
\end{itemize}

About the symbol \#: initially it indicates that a tape cell has not
been visited or written before, but the machine may write that symbol
any time. Often, texts refer to it as the blank symbol and use
$\sqcup$. We often refer to \# as the blank symbol as well.


For a given TM, $\Sigma^*$ consists of three disjunct subsets:
\begin{enumerate}
\item the strings accepted by TM: $L_{TM}$
\item the strings for which TM loops: $\infty_{TM}$
\item all other strings (they are rejected)
\end{enumerate}
These subsets are used in the following definitions.



\begin{definition}[To recognize]
A Turing machine TM recognizes $L_{TM}$.
\end{definition}

Its complementary definition is

\begin{definition}[Turing-recognizable language]
A language $L$ is Turing-recognizable if there exists a Turing machine
TM so that TM recognizes $L$ or alternatively, $L = L_{TM}$
\end{definition}

Here is an example of a recognizable language $L$ and a TM that
recognizes $L$: $\Sigma = \{a,b\}$, $L = \{a\}^*$. Table \ref{turing3}
describes $\delta$.

\begin{table}[ht]
\center
\begin{tabular}{|l|c||l|c|c|}
\hline
$Q$    & $\Gamma$   &  $Q$  &  $\Gamma$ &  LRS \\ \hline
$q_s$  &  a         &  $q_s$&   \#      &  R   \\
$q_s$  &  b         &  x    &    b      &  S   \\
$q_s$  &  \#        &  $q_a$&    -      &  -   \\
x      &  a         &  x    &    a      &  S   \\
x      &  b         &  x    &    b      &  S   \\
x      &  \#        &  x    &    \#     &  S   \\
\hline
\end{tabular}
\caption{A TM recognizing $\{a\}^*$} \label{turing3}
\end{table}
$\infty_{TM}$ is not empty: every string not in $L$ brings the machine
in an infinite loop. But we can also construct a TM for $L$ that always
halts, this is shown in table \ref{turing4}.

\begin{table}[ht]
\center
\begin{tabular}{|l|c||l|c|c|}
\hline
$Q$    & $\Gamma$   &  $Q$  &  $\Gamma$ &  LRS \\ \hline
$q_s$  &  a         &  $q_s$&   \#      &  R   \\
$q_s$  &  b         &  $q_r$&    -      &  -   \\
$q_s$  &  \#        &  $q_a$&    -      &  -   \\
\hline
\end{tabular}
\caption{A TM that decides $\{a\}^*$} \label{turing4}
\end{table}

This distinction is captured in the following definitions:

\begin{definition}[To decide]
A Turing machine TM decides a language $L$, if TM recognizes $L$ and
$\infty_{TM} = \emptyset$.
\end{definition}

\begin{definition}[Turing-decidable language]
A language $L$ is Turing-decidable if there exists a Turing machine that
decides $L$.
\end{definition}

It might not be clear at this point that to recognize and to decide
are two very different things, but we will work towards this
understanding. Still, it should be clear that a decidable language is
also recognizable.

Other terminology for {\em decidable language} is {\em recursive
  language}, and for {\em recognizable language}, one uses {\em
  recursively enumerable}, or sometimes {\em semidecidable}.

\begin{definition}[Co-recognizable/co-decidable]
A language $L$ is \textbf{co-recognizable/co-decidable} if $\overline{L}$
is recognizable/decidable.
\end{definition}

\begin{theorem} \label{corecognizable}
If $L$ is decidable, then $L$ is also co-decidable.
\end{theorem}
\begin{proof}
Let $M$ be the TM deciding $L$; now switch the roles of $q_a$ and $q_r$.
\end{proof}

\begin{theorem} \label{isdec}
If $L$ is recognizable and co-recognizable, then $L$ is decidable.
\end{theorem}
\begin{proof}
Let $M_1$ be the machine that recognizes $L$, and $M_2$ the machine
recognizing $\overline{L}$. The idea is to have a machine $M$ that
lets $M_1$ and $M_2$ run in parallel: as soon as $M_1$ accepts, accept; as soon
as $M_2$ accepts, reject. $M_1$ and $M_2$ can not accept both, and for any
given string, at least one of them halts in the accepting state. This
$M$ decides $L$.

The definition of $M$ is informal, and we should make it more formal:
define $M$ as a 2-tape machine and work out the details.
\end{proof}

\begin{theorem}
Some languages are not recognizable.
\end{theorem}
\begin{proof}
The set of Turing machines is countably infinite, so the set of
recognizable languages can be at most countably infinite. On the other
hand, the set of languages (being ${\cal P}(\Sigma^*)$) is uncountably
infinite. So there must exist languages that are not recognizable.
In fact, most languages are not recognizable.
\end{proof}

What is our plan with Turing machines?

\begin{itemize}
\item we want to further explore the difference between deciding and
  recognizing
\item fit into this picture the regular and context free
  languages
\item study examples of languages that can not be decided or
  recognized by a TM and general techniques for proving this
\end{itemize}

\begin{exercise}
Comment on the following sentences:
\begin{itemize}
\item every string is recognizable
\item every string is decidable
\item every finite language is decidable/recognizable
\item the union/intersection of two recognizable languages is recognizable
\item the intersection of a recognizable and a decidable language is
  decidable
\end{itemize}
\end{exercise}



%===============================================================================
\section{A Graphical Representation of Turing Machines}

Just as FSAs and PDAs, TMs can be visualized by a graph: the vertices
are the states. Figure~\ref{turing2} illustrates it for the TM
deciding the language $\{0^n1^n\mid n \geq 0\}$.

\begin{figure}[h]
	\centering
\includegraphics[%
  width=0.7\linewidth,
  keepaspectratio]{turing2}
\caption{Graphical representation of a Turing machine}
\label{turing2}
\end{figure}

Each state has as many outgoing edges as elements in $\Gamma$ (some
parallel edges are treated together) - except for the accepting and rejecting
state of course. The label represents the $\delta$ of the machine. We
use \_ to indicate that the value at this place is irrelevant: this is
typical for transitions to a halting state.



One can describe in words what the machine does with an input
consisting of only zeroes and ones.

\begin{itemize}
\item if in the start state the head sees a
\begin{itemize}
\item \#: accept
\item 1: reject
\item 0: wipe it out, go to the right, and switch to state $n$; it
  will skip all zeroes
\end{itemize}

\item if in state $n$ (``nul'') the head sees a
\begin{itemize}
\item \#: reject
\item 1: go right to state $e$; it will skip all ones
\item 0: go right
\end{itemize}


\item in state $e$ (``één'')
\begin{itemize}
\item 1: go right
\item 0: reject
\item \#: go left, switch to $v$; it will erase one 1
\end{itemize}

\item in state $v$ (``verwijder'')
\begin{itemize}
\item 0: reject (but actually can't occur)
\item \#: reject (but actually can't occur)
\item 1: erase the 1 (overwrite it by \#); go left on the tape and to
  state $l$; it will find the leftmost symbol of the string
\end{itemize}

\item in state $l$ (``links'')
\begin{itemize}
\item 0/1: go left;
\item \#: go right and to the start state
\end{itemize}

\end{itemize}

As you see, every state has its little something to contribute to the
whole program. Intuitively, what this TM does is identify a word that starts with a $0$ and ends with a $1$, whilst simultaneously removing those end symbols. It then repeats this process on the reduced word, hopefully until both the $0$s and $1$s run out at the same time. The only words that satisfy this are of the form $0^n 1^n$.


The TM in Figure~\ref{turing2} proves that
%
$\{0^n1^n\mid n \geq 0\}$ is a decidable language.


%===============================================================================
\section{Representation and Simulation of TM Computations}

It is often handy to be able to compactly represent a {\em
  configuration} of a TM: it is like a computer memory dump, together
with all registers and the hardware state, so that execution could
restart from that information. For a TM, we can do that because at
each moment, only a finite part of the tape is in use, i.e. is not \#. A string of the form

\begin{equation*}
	\alpha q \beta
\end{equation*}

can represent the tape containing $\alpha\beta$ with to its left and
right only \#; moreover, the machine is in state $q$ and the head is
positioned under the leftmost symbol of $\beta$. $\alpha$ and $\beta$
may contain \#.

A start configuration is always of the form $q_s\alpha$, and an end
configuration is either $\alpha q_a \beta$ or $\alpha q_r \beta$. During the execution of the TM, two successive configurations are
related by $\delta$. Say $\delta(q,b) = (p,c,D)$, with $D\in\{L,S,R\}$. Then valid transitions are:

\begin{align*}
	\alpha q b \beta &\rpijl \alpha p c \beta & (D = S) \\
	\alpha a q b \beta &\rpijl \alpha p a c \beta & (D = L) \\
	\alpha q b \beta &\rpijl \alpha c p \beta & (D = R)
\end{align*}
You can add some transitions for the case where $\alpha$ or $\beta$ is
empty: you might need to generate extra occurrences of \#.


A sequence of configurations from a start to an end configuration -
following $\delta$ - is called a {\em computation history}.

It is reasonably easy to see that the computation of any TM can be
simulated by a program, for instance in Prolog. To make that more
concrete, here is some Prolog code. A configuration $\alpha q \beta$
is represented as a term with principal functor conf/3, as follows:
\begin{equation*}
	conf([a_n, ... , a_2,a_1],q,[b_1,b_2,...,b_m])
\end{equation*}
where $a_1a_2...a_n = \alpha$ and $b_1b_2...b_m = \beta$ and $a_i, b_j \in \Gamma$.\footnote{Look up {\em Huet's Zipper}.}

Here is part of the Prolog program working with configurations:

\begin{verbatim}
       onestep(conf(Alpha,Q,[]), NewConf) :-
                 onestep(conf(Alpha,Q,[#]), NewConf).
       onestep(conf(Alpha,Q,[B|Beta]), conf(Alpha,P,[C|Beta])) :-
                 delta(Q,B,P,C,stop).
\end{verbatim}

\begin{exercise}
Extend the above program, and explain what each clause does.
\end{exercise}

We should also understand that a TM can simulate a Prolog program: to
do that directly would be a lot of work, so here is a classical road
towards the same goal. First prove that Prolog can be implemented on
an Intel machine - that has been done many times, e.g. by the SWI
Prolog implementation. Next prove that an Intel machine can be
simulated by a register machine; the register machine is in the first
place a theoretical construct, but only a small step away from the TM.

That closes the circle: if a TM can simulate any X-program, and an
X-program can simulate any TM, it means that X and the TM have the
same computational power. Careful though: it says nothing about
complexity, just about which languages can be recognized or decided!

We can often use the equivalence between TMs and more recent
programming formalisms.


\begin{exercise}
We often need to combine a number of Turing machines $TM_1, TM_2
... TM_n$ into a new Turing machine TM. We use words like ``{\em TM calls T$M_1$ as a subroutine}'' and ``{\em when $TM_1$ stops in its accepting end state,
let $TM_2$ run on s}''. Make sure you know what this means, i.e. formalize it to some extent.
\end{exercise}




%===============================================================================
\section{Non-deterministic Turing Machines}

We mention non-deterministic TMs for no other reason than that every
book does it: as far as computability goes, non-determinism for TMs
has little interest. They are however important in the context of
complexity.

The signature of $\delta$ is adapted for non-determinism to

~~~~~~~~~~~~~$Q \times \Gamma \rightarrow {\cal P}(Q \times \Gamma
\times \{L,R,S\})$,

i.e. from a given configuration possibly more than one new
configuration can be reached. Two would have been enough actually -
try to understand why. A string can have more than one computation history. A string is accepted if there exists an accepting computation history.
% That is similar to the notion of acceptance in an NFA: one accepting
% path is enough to be accepted.

A non-deterministic Turing machine can be simulated by a
deterministic one. Some books actually describe such a
simulation. For us, it is more natural to reason as follows:
the Prolog program given earlier can be meta-interpreted\footnote{Interpreting the Prolog program means to propose a valid solution to a query. Meta-interpreting means to propose an algorithm that finds these valid solutions.} according to
iterative deepening (or breadth-first) search, and as soon as it arrives in an end state, it stops with the same result. So, if we can simulate a NDTM in Prolog, we can with a DTM. 


%===============================================================================
\section{Languages, ${\cal P}(\N)$, Properties and Terminology}

We have defined the notions recognizable/decidable in terms of
languages over an alphabet. It is equally common to define those
concepts in the context of subsets of $\N$. Those two views are
equivalent, because $\N$ can be seen as $\{0,1\}^*$: it would be a
binary notation of natural numbers (but another one would be OK as
well). So, a subset of $\N$ is a language. Commonly, one reserves the
term {\em (recursive) enumerable} for such sets (or languages), and
the terms {\em decidable} and {\em recognizable} for properties or
decision problems. The distinction is not so important: let $P$ be a
property that elements of a set $V$ might or might not have; we can
define the subsets $\{x \mid x \in V \wedge P(x)\}$. It now becomes
apparent that properties and subsets give an equivalent view on the
issues.

The word {\em recursive} often triggers the image of a function
(method, predicate, procedure ...) activating itself, or a data type
defined in terms of itself (a list, a tree, ...). But the {\em
  recursive} in the term {\em recursive enumerable} is not related to
that.



%===============================================================================
\section{Encoding}

When we describe a language informally, there is no need to bother
with an encoding. We can talk about {\em planar graphs} for instance,
without thinking in terms of a particular alphabet. Still, when we
want to describe an algorithm dealing with planar graphs, then we need
an alphabet and a mapping from graphs to strings over that alphabet.
Such a mapping is an encoding. Encodings need to be {\em
  reasonable}. An encoding must have at least the following three
properties (related to encoding of graphs):
\begin{itemize}
\item one graph encodes to one string (encoding is a function)
\item two {\em different} graphs encode to two different strings (the function is injective)
\item it must be decidable whether a string is the encoding of a graph
  and of which one
\end{itemize}
A reasonable encoding should not introduce extra information.

Some examples:
\begin{enumerate}
\item
You are studying prime numbers. You choose a unary representation: the
alphabet is $\{@\}$. You represent 7 by 7 times the symbol @, i.e. by
@@@@@@@

That is reasonable.

\item
You are studying prime numbers. You choose a binary representation: the
alphabet is $\{@,!\}$. You represent 9 by !@@!

That is reasonable.


\item
You are studying prime numbers. You choose a two-part representation of
numbers: the first part indicates whether the number is prime or not;
if it is, this first part equals $p$, otherwise $n$. The second part
of the representation is the unary representation. So 7 is
represented as p@@@@@@@ and 9 as n@@@@@@@@@

That is not reasonable: your encoding has extra information.

Maybe you think that having more information is a good thing. For instance,
you can now decide whether a number is prime by inspecting its first
symbol only. You have a constant time check for primality!

However, consider the input string p@@@@. The first letter tells you
it is prime, but the rest of the string denies that ... In fact, the
string does not represent a number, but you can't tell unless you
check whether the sequence of @ has prime length ... So in fact, the
extra information that seems to be there, needs to be checked. This
check has the same price as if the extra information would not have
been there.

\end{enumerate}


As far as computability is concerned, the encodings in (1) and (2)
above are equivalent: you can transform one into the other with an
algorithm implemented on a TM.

As far as complexity theory is concerned, the two encodings are not
equivalent! E.g. adding two numbers in unary notation is $O(n)$ (with $n$
the value of the numbers), while in binary encoding, it is
$O(\log(n))$.

We denote the encoding of an object $S$ by $\langle S \rangle$. When
we encode a sequence of objects $S_i$, we use $\langle
S_1,S_2,... \rangle$.

\begin{exercise}
About encodings:
\begin{itemize}
\item Find reasonable encodings for trees, XML-documents, ...

\item Find some unreasonable encodings for the same objects, or for others.

\item For a function, say from $\N$ to $\N$, one can also encode the
output. Your professor gave you as project to compute, on input $i$, the
$i^{th}$ prime number. You finished very quickly: for input $i$, you
produced output $i$, arguing that $i$ in the output is just the
encoding of the $i^{th}$ prime number. Your professor was not
happy. Was that reasonable?

\end{itemize}
\end{exercise}

In the next chapters, languages that consist of encodings of TMs (denoted $\langle M\rangle$) occur often. Say we have a predicate $P$ over TMs, and that we have the language:
\begin{equation*}
	L = \{\langle M\rangle \mid \text{$M$ is a TM and } P(M)\}.
\end{equation*}
One could wonder what the meaning of $\overline L$ is. On the one hand, we would like $L$ and $\overline L$ to partition $\Sigma^*$ (the set of all strings built from the encoding alphabet). But clearly, for some encodings, there are strings in $\Sigma^*$ that are bogus, and it's often inconvenient to consider those part of $\overline L$: we will usually write (or tacitly assume) that
\begin{equation*}
	\overline L = \{\langle M\rangle \mid \text{$M$ is a TM and } \neg P(M)\}
\end{equation*}
... whilst other textbooks might add the set $\{s\in\Sigma^* \mid \text{$\neg\exists M: M$ is a TM and $s = \langle M\rangle$} \}$ to it. Since it is decidable whether a string is a valid encoding (for reasonable encodings), our convention doesn't change the computability, but it's good to be aware that $L \cup \overline L \neq \Sigma^*$ in our case (unless the encoding is surjective, i.e. all strings built from encoding symbols are valid encodings).

%===============================================================================
\section{Universal Turing Machines}
We just mentioned encodings of TMs. A TM can be described completely by its transition table (plus some
conventions to identify the start and end states). One can encode the
transition table, by using numbers for the states (from 1 to ...),
numbers for the symbols, and a number for $L$, $R$ and $S$. These numbers
would then be encoded in e.g. unary, and one would just need a new
symbol for delimiting the different numbers. We put the encoding of
the transition table on the first tape of a UTM (the program tape). On
its second tape (the memory), we write an input string (using the same
encoding for the symbols).  UTM may have even more tapes if that makes
life easier.

UTM has its own program: it uses the program and memory tape as input,
it simulates the behavior of the TM (whose $\delta$ is input) on the
given string. As soon as TM would stop in its $q_a$ or $q_r$, UTM
makes a transition to its $q_a'$ or $q_r'$.

We could provide more detail about the construction above, but for us,
it is detailed enough: the Universal Turing machine can simulate any
(properly encoded) TM: UTM could even start by checking whether the
tapes contain a proper encoding.

Do not get confused by one of the previous sections in which you were
asked to think about a $TM_1$ that uses another $TM_2$ as a
subroutine: in that case, the states of $TM_2$ are a subset of the
states of $TM_1$. This is not the case in the UTM: the UTM has a fixed
number of states, independent of the TM it is simulating, and the
encoded states of the TM are not states of the UTM. As Guy L. Steele,
Jr. said: {\em One person's data is another person's program.}


Is a UTM {\em big}? Claude Shannon showed one can trade states for
symbols in a TM - see page \pageref{twosymbols}. So it
has become a bit of a habit to express the size of a TM as
$(|Q|,|\Gamma|)$, or even just the product $|Q| \times |\Gamma|$. In
1956 Marvin Minsky\footnote{Turing award 1969} constructed a (7,4)
UTM. In the meantime, it has been proven that a particular (2,3)
machine is universal -- but it is a bit controversial.  By the way, (2,2) TMs
cannot be universal.

%###############################################################################
\chapter{Computability}

%===============================================================================
\section{The Halting Problem}\label{halting}

Informally, the question is this: given a Turing machine $M$ and a
string $s$, is it possible to determine whether $M$ stops on input $s$?
We don't care whether $M$ accepts or rejects $s$: in both cases, $M$ stops.
So, the question asks whether $M$ goes into a loop or not. We want this
question to be solved by an algorithm, so we actually want a
Turing machine $H$ that {\bf decides} the language
\begin{equation*}
	H_{TM} = \{\langle M,s \rangle \mid \text{$M$ is a TM and stops on input $s$, i.e. $s\notin\infty_M$}\}.
\end{equation*}

We will show that such a machine $H$ does not exist, by showing that $H_{TM}$ is
undecidable. We already knew that such languages exist, but $H_{TM}$ is our
first concrete example.

A related problem is the acceptance problem for Turing machines. The associated language is
\begin{equation*}
A_{TM} = \{\langle M,s \rangle \mid \text{$M$ is a TM and $s\in L_M$}\}.
\end{equation*}

$A_{TM}$ is the first language for which we prove that it is undecidable.  % [NOTE TB] The traditional proof for H_TM being undecidable is much nicer, and a reduction from H_TM to A_TM exists (a decider for A_TM on M and its complement identifies the language of halting).

\begin{theorem}
$A_{TM}$ is undecidable.
\end{theorem}
\vspace{-0.75em}
\begin{proof}
We prove this {\em by contradiction}.

Assume that a decider $B$ for $A_{TM}$ exists. The fact that $B$ is a decider for $A_{TM}$ means that given
an input $\langle M, s \rangle$: 
\begin{itemize}
\item $B$ accepts iff $M$ accepts $s$, and
\item $B$ rejects iff $M$ rejects $s$ or loops.
\end{itemize}

We now construct a Turing machine $C$ which we call the \emph{contradiction machine}. It takes a Turing machine $M$ as input, constructs the tuple $\langle M,s\rangle$ with $s = \langle M\rangle$, applies $B$ to this, and inverts the answer. In short:
\begin{align*}
	C(\langle M \rangle) &= \mathit{opposite}(B(\langle M, \langle M \rangle\rangle)) \\
	\mathit{opposite}(\mathit{accept}) & = \mathit{reject} \\
	\mathit{opposite}(\mathit{reject}) & =  \mathit{accept}.
\end{align*}
Since $C$ is a Turing machine that takes (an encoding of) a Turing machine as input, we
can of course apply $C$ to itself.
We have that:
\begin{equation*}
\begin{array}{crcl}
                 & C(\langle C \rangle) & = & \mathit{reject} \\
\Leftrightarrow  & \mathit{opposite}(B(\langle C, \langle C \rangle\rangle)) & = & \mathit{reject} \\
\Leftrightarrow  & B(\langle C, \langle C \rangle\rangle) & = & \mathit{accept} \\
\Leftrightarrow  &  C(\langle C \rangle)& = & \mathit{accept}
\end{array}
\end{equation*}
Clearly, this is inconsistent. Hence, $C$ cannot exist, which means that $B$ cannot exist.
So, $A_{TM}$ is undecidable.
\end{proof}

\begin{theorem}
$H_{TM}$ is undecidable.
\end{theorem}
\begin{proof}
Suppose $H_{TM}$ is decidable by $H$. We define a decider $B$ for
$A_{TM}$ as follows: on input $\langle M,s \rangle$:
\begin{itemize}
\item $B$ runs $H$ on $\langle M,s \rangle$
\item if $H(\langle M,s \rangle) = \mathit{accept}$, then $B$ lets $M$ run on $s$ and relays the result to its own output;
\item if $H(\langle M,s \rangle) = \mathit{reject}$ then $B$ rejects $\langle M,s \rangle$.
\end{itemize}
Convince yourself that $B$ decides $A_{TM}$, in particular that it
does not loop.

Since a decider for $A_{TM}$ is non-existent, $H$ does not exist, so
$H_{TM}$ is undecidable.
\end{proof}

\begin{theorem}
$H_{TM}$ is recognizable.
\end{theorem}
\begin{proof}
On input $\langle M,s \rangle$, the recognizer $H$ for $H_{TM}$ simply
runs $M$ on $s$: if it stops (either end state), $H$ accepts its input;
otherwise it keeps running ... and that is OK.
\end{proof}

Quite often, when we construct or define a recognizer, it has an
accepting state, but no point at which it rejects.

\begin{theorem}
$A_{TM}$ is recognizable.
\end{theorem}
\begin{proof}
Similar to the recognizer for $H_{TM}$.
\end{proof}


As a direct consequence we have

\begin{corollary}
$\overline{A_{TM}}$ and $\overline{H_{TM}}$ are unrecognizable.
\end{corollary}
\begin{proof}
This follows from Theorem~\ref{isdec} and the fact that $A_{TM}$ is
recognizable and not decidable. Same for $H_{TM}$.
\end{proof}





\section{The Enumerator Machine}\label{enumerator}

The enumerator machine seems to be the machine as originally described
by Alan Turing in his 1936 publication: he was interested in
generating decimal expansions of {\em computable real numbers}. The
link with recognizable languages is quite direct. We had to wait with
introducing the enumerator until we dealt with the halting problem.

\begin{figure}[h]
	\centering
	\includegraphics[  width=0.7\linewidth,  keepaspectratio]{enumerator1eng}
	\caption{Schema of an enumerator machine\label{enumerator1}}
\end{figure}

In Figure~\ref{enumerator1} you can see that the enumerator is like a
Turing machine with some extras
\begin{itemize}
\item an enumerator state $q_e$
\item an output tape
\item an output marker
\end{itemize}
The signature of its $\delta$ is
%
$Q \times \Gamma \rightarrow Q \times \Gamma \times \Gamma_\epsilon \times \{L,R,S\}$.
The last $\Gamma_\epsilon$ is meant to be the symbol written at that
transition to the output tape.

The machine \emph{starts with an empty tape} (unlike a TM) and empty output tape, and starts working from the
usual $q_s$. Whenever something is written on the
output tape, the write head moves one position to the right. Whenever
the machine gets in state $q_e$, the output marker is written on the
output, the machine goes to state $q_s$ and the machine continues.

It is possible that the enumerator does not stop, and outputs one
after another from a set of strings (separated by output markers):
this set could be finite, or infinite (can it be uncountably
infinite?). It is possible that the enumerator does not stop, and
keeps working on a particular output string, maybe even the first one!
It is also possible that the machine stops after a while and leaves a (finite!) set of strings on
the output separated by output markers.

Whichever of the above scenarios, it makes sense to talk about the set
of (finite) output strings produced by an enumerator: that is the
language determined by the enumerator, or enumerated by the
enumerator. An enumerator is allowed to produce the same output
string multiple times. We can now prove:

\begin{theorem}
The language enumerated by an enumerator is recognizable, and every
recognizable language can be enumerated by an enumerator.
\end{theorem}
\begin{proof*}
\begin{itemize}
	\item[$\Rightarrow$] 
	Given an enumerator Enu for $L$, we describe informally a recognizer
	  $M$ for $L$: $M$ uses Enu as a subroutine as follows:
	\begin{itemize}
	\item On input $s$, $M$ starts Enu. Any time Enu is in its $q_e$ state, M
	inspects the most recently produced string on the output tape of
	Enu. If it equals $s$, $M$ accepts $s$. Otherwise Enu continues.
	\end{itemize}

	\item[$\Leftarrow$] 
	Suppose $M$ recognizes $L$. We construct an enumerator Enu for $L$ with the following ingredients:
	
	\begin{itemize}
	\item
	a $TM_{gen}$ that given a number $n$ puts the first $n$
	strings from $\Sigma^*$ on a tape: $s_1, s_2, ... s_n$
	
	\item
	a $TM_n$ that executes $n$ transitions
	of $M$ on each of the $n$ strings: if a string $s_i$ is accepted within that budget, it is written
	on the output tape of Enu
	
	\item
	a $TM_{driver}$ that generates the numbers 1,2,... one after the
	other and for each of them calls $TM_{gen}$ and $TM_n$\vspace{-2em}
	\end{itemize}
\end{itemize}
\end{proof*}

Why did we need acquaintance with the halting problem? Naively, we
would have proposed the following procedure to make an enumerator Enu
for the recognisable language of a Turing machine $M$:

\begin{itemize}
\item
generate the strings from $\Sigma^*$ in any order:
$s_1, s_2, \hdots$

\item
give each $s_i$ as input to $M$ and if $M$ accepts, output $s_i$
\end{itemize}

That does not work, because $M$ might loop on some $s_i$, and thanks to
the halting problem, we understand there is no way to know this in
advance. Suppose $s_{i+1}$ belongs to $L_M$, then our thus constructed
Enu would not enumerate it.



\section{Decidable Languages}

\subsection{Related to regular languages}

The sentence {\em regular languages are decidable} might be ambiguous:
we must specify precisely what is input to the decider. So we refine
that sentence, depending on how the regular language itself is given.


\begin{itemize}
\item
$A_{DFA} = \{\langle D,s \rangle\mid \text{$D$ is a DFA and $D$ accepts $s$} \}$
% \item
% $A_{NFA} = \{\langle N,s \rangle|N~is~a~NFA,~and~N~accepts~s\}$
\item $A_{RegExp} = \{\langle RE,s\rangle\mid \text{RE is a regular expression and RE generates $s$}\}$
\end{itemize}

\begin{theorem} \label{reglandec}
$A_{DFA}$
%, $A_{NFA}$ 
and $A_{RegExp}$ are decidable.
\end{theorem}
\begin{proof}
The proof is constructive.
\begin{itemize}
\item
The decider $B$ has input $\langle D,s \rangle$. $B$ simulates $D$ on $s$. If
$D$ accepts $s$, $B$ stops in its $q_a$. If $D$ rejects $s$, $B$ stops in its
$q_r$. Looping does not occur.

%  \item
%  The simulation of an NFA can loop, unless one performs loop detection
%  ... We chose an easier path: the decider B has input $\langle D,s
%  \rangle$. B transforms the NFA D to a DFA (see the algorithm in
%  Section~\ref{detfsa}). Now use the above simulation.

\item
Given input $\langle RE,s \rangle$, first transform the RE to a DFA
and proceed as above.
\end{itemize}\vspace{-1.5em}
\end{proof}

The technicalities of reductions are introduced only later, but
above, we used the principle of reduction twice.

We can also prove that any regular language is decidable. Try to
understand the subtle difference with the statement in
Theorem~\ref{reglandec}. (there is a hint in the proof)


The following three questions about a language are quite popular:
\begin{itemize}
\item does the language contain the empty string?
\item is the language empty?
\item are two given languages equal?
\end{itemize}

Pertaining to regular languages, we can construct a decider for each
of these questions, but once more, we need to specify what is the
input to the decider. Instead of constructing the decider as a TM, we
will informally explain its working. Make sure that all steps are
realizable as a TM.


The first question is trivial, because $A_{DFA}$ is decidable.

\begin{theorem}
$E_{DFA} = \{\langle DFA \rangle\mid L_{DFA} = \emptyset\}$ is decidable.
\end{theorem}
\begin{proof}
Transform the DFA to a minimal $DFA_{min}$ accepting the same language.
If $L_{DFA} = \emptyset$ then $DFA_{min}$ is isomorphic with ... (draw that
machine). Deciding that is easy.\footnote{There are many other ways to prove this. We used some theory we studied before, but is also a bit overkill: you are invited to find shorter and/or more elegant ways.}
\end{proof}

\begin{theorem}
$EQ_{DFA} = \{\langle DFA_1,DFA_2 \rangle\mid L_{DFA_1} = L_{DFA_2}\}$ is decidable.
\end{theorem}
\begin{proof}
We use some algebraic properties of the set of DFAs.

From $DFA_1$ and $DFA_2$, construct $DFA_\Delta$ that accepts the
symmetric difference between $L_{DFA_1}$ and $L_{DFA_2}$. Then decide
whether $DFA_\Delta$ corresponds to the empty language using the previous
theorem. (Did you see the reduction?)
\end{proof}

\begin{exercise}
Find other proofs for the theorems above.
\end{exercise}


\subsection{Related to context free languages}

The questions are similar to the ones in the previous section, and can
be formulated either by giving the CFG or the PDA for the context free
language at hand. We show it for the grammar only.

\begin{theorem} \label{acfg}
$A_{CFG} = \{\langle G,s \rangle\mid \text{$G$ is a CFG and $s\in L_G$}\}$ is
    decidable: {\em acceptance} of a string by a CFG is decidable.
\end{theorem}
\begin{proof}
A naive proof idea consists in using the CFG to generate strings, and
accepting as soon as $s$ is generated. You can see why this does not work:
when do you reject? We have a recognizer, not a decider.

It would be convenient if the CFG were in Chomsky Normal Form: we
would then have an upper bound on the length of the derivation for a
given string.

So, for input $\langle G,s \rangle$, first convert G to its Chomsky
Normal Form. Generate all possible strings with a derivation length
$2|s|-1$: there are only finitely many of them. If $s$ is one of them,
accept, otherwise reject.
\end{proof}

\begin{theorem}
$E_{CFG} = \{\langle G \rangle\mid \text{$G$ is a CFG and $L_G = \emptyset$}\}$ is decidable: {\em emptiness} of a CFL is decidable.
\end{theorem}
\begin{proof}
We informally describe an algorithm transforming $G$ to a form in which
taking the decision is easier.
\begin{itemize}
\item
for a rule $A \rightarrow \alpha$ with $\alpha$ only terminal symbols
\begin{itemize}
\item remove all rules with A at the left side
\item replace the occurrences of A in any right side by $\alpha$
\end{itemize}


\item
keep doing this until
\begin{itemize}
\item the start symbol is removed: reject, because the start symbol
  can derive a string
\item there are no rules of the required form: accept, because the
  language is empty
\end{itemize}
\end{itemize}
\end{proof}

Be careful with the above proof: the grammar is transformed to a
non-equivalent one!

\begin{exercise}
Write down a grammar that determines the empty language and apply the
above procedure to it. Did you learn something about Prolog programs
that have no answers?
\end{exercise}

\begin{theorem} \label{escfg}
$ES_{CFG} = \{\langle G \rangle\mid \text{$G$ is a CFG and $\eps\in L_G$}\}$ is
    decidable: it is decidable whether a CFG generates the empty string.
\end{theorem}
\begin{proof}
Transform the CFG to its Chomsky Normal Form. If it now contains the
rule $S \rightarrow \epsilon$, accept, otherwise reject.
\end{proof}

\begin{theorem}
Every CFL is decidable.
\end{theorem}
\begin{proof}
Here, the question is to prove the existence of a decider $B_G$ that
can decide for each string whether it belongs to G or not. Appreciate
the difference with $A_{CFG}$. Work out the details yourself.
\end{proof}

The remaining one is $EQ_{CFG}$, i.e. if we get two CFGs, can we
decide whether they have the same language? The decider for $EQ_{DFA}$
relied on the symmetric difference of regular languages. We can not
use the same idea for CFLs, because they are not closed under
complement or intersection.

\begin{exercise}
Do this yourself:
\begin{itemize}
\item Prove that $EQ_{CFG}$ is undecidable (you may want Theorem~\ref{allcfg} for this).

\item Prove that $\overline{EQ_{CFG}}$ is recognizable.

\item What does that mean for $EQ_{CFG}$?

\item Prove that Theorem~\ref{escfg} follows directly from Theorem~\ref{acfg}.
\end{itemize}
\end{exercise}

% los eindje in de cursus: nergens wordt bewezen dat $EQ_{CFG}$ niet-recognizable is

\section{Undecidable Languages}

In Section~\ref{halting} we proved that $A_{TM}$ and $H_{TM}$ are not
decidable. Here are some more of the same kind.

\begin{theorem} \label{nonreductie1}
$E_{TM} = \{\langle M \rangle \mid \text{$M$ is a TM and $L_M = \emptyset$}\}$ is not
     decidable: it is not decidable whether a Turing machine accepts
     no input.
\end{theorem}
\begin{proof}
Assume that $E_{TM}$ is decidable, then there exists a decider $E$ for
it. We now describe a decider $B$ for $A_{TM}$ using $E$:
$B$ receives as input $\langle M,s \rangle$ and works in two steps.
\begin{enumerate}
	\item It constructs a machine $M_{M,s}$ whose internal mechanism is as follows: on input
	$w$,
	\begin{itemize}
		\item if $w \neq s$, reject
		\item else run $M$ on $s$ and relay the result to the output.\footnote{We don't often acknowledge that $M$ possibly loops, but it's good to know that this is also allowed. By relaying the output of $M$ to the output of $M_{M,s}$, the latter loops when the former does on a given input, without knowing it -- if it did always know, then this would violate the properties of the halting problem.}
	\end{itemize}
	
	\item It runs $E$ on input $\langle M_{M,s} \rangle$, and
	\begin{itemize}
		\item if $E$ accepts $\langle M_{M,s} \rangle$, then $B$ rejects its
		  input $\langle M,s \rangle$; indeed: $M_{M,s}$ determines the empty
		  language, so $M$ has not accepted s
		\item if $E$ rejects $\langle M_{M,s} \rangle$, $B$ accepts its input: since  $M_{M,s}$ is not empty, $M$ accepts s
	\end{itemize}
\end{enumerate}
It follows that $B$ is a decider for $A_{TM}$, which is impossible, so
$E$ does not exist, so $E_{TM}$ is not decidable.
\end{proof}

We can define the $E_{TM}$ problem slightly differently, as an instance
of a more general problem: does a given machine $M$ determine a language
from a given set? So we have
\begin{equation*}
	\text{IsIn}_{TM,S} = \{\langle M \rangle\mid L_M \in S\}.
\end{equation*}
Then $E_{TM} = \text{IsIn}_{TM,\{\emptyset\}}$. Another instance of the general problem: does a given Turing machine $M$ determine a regular language, i.e. is $\text{IsIn}_{TM,RegLan}$ decidable? We denote that problem as $REGULAR_{TM}$.

\begin{theorem}
$REGULAR_{TM}$ is not decidable.
\end{theorem}
\begin{proof}
Suppose Turing machine $R$ decides $REGULAR_{TM}$. Fix two symbols
from the alphabet, say 0 and 1. Make a decider $B$ for $A_{TM}$ as
follows: on input $\langle M,s \rangle$ $B$ does the following:
\begin{itemize}
\item it constructs a helper machine $H_{M,s}$ that on input $x$ does
  the following
\begin{itemize}
\item if $x$ is of the form $0^n1^n$, accept
\item else let $M$ run on $s$; return the same answer
\end{itemize}

\item then $B$ lets $R$ run on  $\langle H_{M,s} \rangle$
\item if $R$ accepts, accept; if $R$ rejects, reject
\end{itemize}

First note that $H_{M,s}$ never runs: it is there only as input for
$R$. Finishing the proof:


$H_{M.s}$ either accepts the non-regular language $\{0^n1^n\}$
or the regular language $\Sigma^*$. So, $B$ accepts $\langle
M,s \rangle$ iff $R$ accepts $\langle H_{M,s} \rangle$, iff $H_{M,s}$
accepts $\Sigma^*$, iff $M$ accepts $s$. It follows that $B$ is a
decider for $A_{TM}$, which is impossible, so $R$ does not exist, so
$REGULAR_{TM}$ is not decidable.
\end{proof}

\begin{theorem} \label{reduction1}
$EQ_{TM}$ is not decidable.
\end{theorem}
\begin{proof}
We already know that $E_{TM}$ is not decidable. $E_{TM}$ is a special
case of $EQ_{TM}$, in which the second machine is $M_{\emptyset}$. It is
clear that if we could decide the equality of two arbitrary languages,
we would also be able to decide about one arbitrary language
and the empty one. So, we arrive at a contradiction by assuming that
$EQ_{TM}$ is decidable.
\end{proof}

The above proof technique is studied in more detail in
Section~\ref{mappingreduction}.

The following is about context-sensitive languages. We first define
the machinery needed to decide them:

\begin{definition}[Linear Bounded Automaton]
A \textbf{linear bounded automaton (LBA)} is a (non-deterministic) Turing machine
that reads and writes only on the part of the tape that contained the
initial input.
\end{definition}

The name of the automaton might strike you as weird. An equivalent
definition allows the machine to use a portion of the tape that is
larger by a constant factor $f$ than the input: this $f$ is
independent of the input. We mentioned earlier that such machines can
decide context sensitive languages. You can intuitively work that out
by adhering to the alternative definition of context sensitive grammar
on page \pageref{altdefcs}, trying out a bottom-up parse, and notice
that you do not need extra space.

There is no decision procedure for deciding whether a given Turing
Machine is actually an LBA (adapt the proof for $REGULAR_{TM}$). But a
TM can be turned into an LBA by adding a marker at the left and right
of the input, and adapting the transition table so that this marker is
never crossed.
\begin{exercise}
Show with an example that in general you have changed the language.
\end{exercise}

The acceptance problem for LBAs is
\begin{equation*}
	A_{LBA} = \{\langle M,s \rangle \mid \text{$M$ is an LBA and $s\in L_M$} \}.
\end{equation*}
The following might surprise you ...

\begin{theorem}
$A_{LBA}$ is decidable.
\end{theorem}
\begin{proof}
Consider the configurations that can occur during the execution of
an LBA on an input with length $n$. Using $q$ for the number of states
of the LBA, and $b$ for the size of the tape alphabet, we can quickly
compute the number of possible tape contents: it is bounded by
$b^n$. The tape head can be on any cell of the allowed portion of the
tape, so the upper bound for the number of configurations is $qnb^n$.

A decider $B$ for $A_{LBA}$ can now be constructed as follows:
on input $\langle M,s \rangle$, $B$ performs the following
\begin{itemize}
\item it computes $Max = qnb^n$
\item it simulates $M$ on $s$ for at most $Max$ steps
\item if $M$ accepted in the mean time, accept
\item if $M$ rejected in the mean time, reject
\item if $M$ has not yet stopped, it means that $M$ loops and does not
  accept: reject\vspace{-1.5em}
\end{itemize}
\end{proof}

\begin{theorem}
$E_{LBA} = \{\langle M\rangle \mid \text{$M$ is an LBA and $L_M = \emptyset$}\}$ is not decidable.
\end{theorem}
\begin{proof}
We first show that for a given Turing machine $M$ and string
$s$, we can construct an LBA that can decide about a finite sequence
of configurations of $M$ whether it is an accepting computation
history for $s$. A sequence of configurations can be put on a tape
easily as in the figure.
\begin{figure}
\centering
\includegraphics[%
  width=0.5\linewidth,
  keepaspectratio]{comphistband}
  \caption{Computation history for a transtion $\delta(q_4,c) = (q_7,b,R)$.}
\end{figure}

To check whether a sequence of configurations is an accepting computation
history for $s$, the following is needed:
\begin{itemize}
\item check that two subsequent configurations are connected through
$\delta$
\item check that the first configuration is of the form $q_ss$
\item check that the last configuration contains $q_a$
\end{itemize}

It should be clear - at least intuitively - that these checks need at
most a constant amount of extra space, meaning that the decision can
be taken by an LBA. We construct the LBA in such a way that it accepts
the accepting computation history for $s$, and rejects every other
input.  We can now start the proof.

Suppose we have a decider $E$ for $E_{LBA}$. We construct a decider
$B$ for $A_{TM}$ as follows. On input $\langle M,s \rangle$ $B$
performs the following actions:
\begin{itemize}
\item it constructs the LBA $A_{M,s}$ as above described: this LBA can
  decide whether a given string is an accepting computation history
  for $M$ on input $s$
\item give $\langle A_{M,s} \rangle$ to $E$: if $E$ accepts, reject;
otherwise accept
\end{itemize}
$B$ decides $A_{TM}$ because $B$ accepts $\langle M,s \rangle$ iff $E$
rejects $\langle A_{M,s} \rangle$, iff $A_{M,s}$ accepts at least one
string, iff there exists an accepting computation history for $M$ on
$s$. The latter is equivalent with $M$ accepts $s$.

So, $B$ does not exist, so neither can $E$, and as a conclusion:
$E_{LBA}$ is not decidable.
\end{proof}

\begin{theorem}[Undecidability of some machines]
	A small summary of the differences and similarities between PDAs, LBAs
	and TMs:
	\begin{itemize}
		\item acceptance and halting
		\begin{itemize}
			\item Decidable: PDA and LBA
			\item Undecidable: TM
		\end{itemize}

		\item emptiness
		\begin{itemize}
			\item Decidable: PDA
			\item Undecidable: LBA and TM
		\end{itemize}
	\end{itemize}
\end{theorem}

To conclude for now: it is not decidable whether a CFG generates all
strings from $\Sigma^*$, i.e. $ALL_{CFG} = \{\langle G \rangle\mid L_G =
\Sigma^*\}$ is not decidable. That is a bit weird, as $E_{CFG}$ is
decidable. Then again, complement is not internal for CFLs ...

\begin{theorem}\label{allcfg}
$ALL_{CFG}$ is not decidable.
\end{theorem}
\begin{proof}
No proof this year.  % [NOTE TB] A textbook is timeless! Communicate this in class!
\end{proof}

\begin{exercise}
Prove that $ALL_{RegExp}$ is decidable.
\end{exercise}
We should make one thing explicit before we move on. To prove a language's exact computability class -- e.g.\ for the languages we mentioned above -- you'll likely need two proofs: two for recognisability, or one for undecidability and one for recognisability. We just proved a bunch of undecidabilities, but some of those languages are still recognisable while others aren't. Conversely, if you find a claim (or even a proof) somewhere of a language's recognisability, be careful about whether it is strict or not. You can easily prove that $A_{DFA}$ is recognisable, but it's not \emph{just} that; to be precise, you either need a proof that $A_{DFA}$ or $\overline{A_{DFA}}$ is decidable, or that the latter is also recognisable.

\section{Some Trivia on Languages}
\subsection{Hierarchy}
Let $Dec$ denote the set of decidable languages, and $Rec$ the
recognizable languages. Here are some inclusions, all strict:
\begin{equation*}
	RegLan \subset DCFL \subset CFL \subset Dec \subset Rec \subset {\cal P}(\Sigma^*)
\end{equation*}

\subsection{Operation closure}
A somewhat unusual operation on languages is \emph{inversion}. Denote the string obtained by writing the symbols of $s$ in reverse order with $\hat{s}$. We define $\widehat{L} = \{\hat{s}\mid s \in L\}$.

Now, some closure properties:
\begin{center}
\begin{tabular}{r|ccccc}
	   & $\cup$ & $\cap$ & $\overline{L}$ &  $\widehat{L}$ & $\cdot$\\\hline
RegLan & X & X & X & X & X\\
DCFL   &   &   & X &   &  \\
CFL    & X &   &   & X & X\\
Dec    & X & X & X & X & X\\
Rec    & X & X &   & X & X\\
\end{tabular}
\end{center}
You can read this table from the perspective of the languages, but it's more memorable to consider its transpose:
\begin{itemize}
	\item DCFL is the only class not closed under union, inversion and concatenation;
	\item DCFL and CFL are the only classes not closed under intersection;
	\item CFL and Rec are the only classes not closed under complement;
\end{itemize}
All language classes closed under complement match in their closure under union and intersection due to De Morgan's laws: $L_1 \cup L_2 = \overline{\overline{L_1} \cap \overline{L_2}}$ and its dual $L_1 \cap L_2 = \overline{\overline{L_1} \cup \overline{L_2}}$.

\begin{example}
The language $L = \{ba^mb^nc^k\mid m \neq n\} \cup \{ca^mb^nc^k\mid n \neq k\}$ is
deterministic context free (construct its DCFG) but $\widehat{L}$ is
not.
\end{example}

\section{Countable}

A set $V$ is countable if there exists a bijection between $V$ and (a
part of) $\N$. This definition is about {\em existence} and it does
not care about the ease with which the bijection can be computed: as
far as the definition goes, there is no need for that.

Still, in the context of computability, we care about such bijection
being computable by a Turing machine. We have indeed used
constructions that use a TM that generates the strings of $\Sigma^*$
one by one. It is thus not enough that $\Sigma^*$ is countable, it
needs to be (effectively) enumerable (by a TM). You should be able to prove at
this point that there exist subsets of $\Sigma^*$ that are
not enumerable, and give concrete examples as well.

So, what is the connection between being generated by an enumerator
machine, and effectively enumerable? The enumerator is allowed to
generate duplicates. We can prevent it from doing so by modifying it a
little: any time the enumerator arrives in its enumerator state, we
check whether the most recently generated string is really new: if
not, we erase it. Now the enumerator effectively enumerates all its
strings. Since every language generated by an enumerator is recognizable,
and since we proved that there exists a non-recognizable language $L$,
it follows that this $L$ \emph{can be counted, but not enumerated}.
\begin{exercise}
Construct languages that are countable, but not enumerable.
\end{exercise}


\section{Rice's Theorem}

We have seen some concrete examples of undecidable languages, often by
reduction to $A_{TM}$: every time, we found a new little trick so that
a supposed decider for a language could be used in a decider for
$A_{TM}$ and lead to contradiction. So, Rice's theorem comes as a
relief: in one go, it proves that a whole lot of languages (how many?)
are undecidable.

Consider the set of Turing machines (for convenience over a fixed
alphabet). A property $P$ of the Turing machines splits the set in two
parts: the machines that have property $P$, denoted
%
$Pos_P = \{M\mid P(M) = \mathit{true}\}$, and the machines that do not have the
property, denoted
%
$Neg_P = \{M\mid P(M) = \mathit{false}\}$.

\begin{definition}[Non-trivial property]
A property $P$ of TMs is {\em non-trivial} if
\begin{equation*}
	Pos_P \neq \emptyset \quad\wedge\quad Neg_P \neq \emptyset
\end{equation*}
or in words: there is at least one machine that has the property and one that doesn't.
\end{definition}

\begin{definition}[Semantic property]
The property $P$ is {\em semantic} if
\begin{equation*}
	L_{M_1} = L_{M_2} \Longrightarrow P(M_1) = P(M_2)
\end{equation*}
or in words: equivalent machines have equivalence in $P$.
\end{definition}

\begin{exercise}
~
\begin{itemize}
\item Invent some examples of semantic, non-trivial
properties of TMs.

\item Invent some examples of properties of TMs that are not semantic.

\item For a given non-trivial and semantic property $P$, how many
machines have $P$? How many don't?
\end{itemize}
\end{exercise}



\begin{theorem}[Theorem I by Rice]
Let $P$ be a non-trivial, semantic property of Turing
Machines. $Pos_P$ (and $Neg_P$) is undecidable.
\end{theorem}
\begin{proof}
Suppose $M_\emptyset$ (a machine deciding the empty language) does not
have property $P$ - if not, replace $P$ by its negation. Since $P$ is
non-trivial, there also exists a machine $X$ having $P$ - denote
its language by $L_X$. Suppose $Pos_P$ were decidable by a decider
$B$. We will use $B$ to construct a decider $A$ for $A_{TM}$.

$A$ has as input $\langle M,s \rangle$ and does the following:
\begin{itemize}
\item construct a helper machine $H_{M,s}$ that on input $x$:

\begin{itemize}
\item runs $M$ on $s$
\item if $M$ accepts $s$, lets $X$ run on $x$ and returns its result
\end{itemize}

\item now give $H_{M,s}$ to $B$
\item if $B$ accepts $H_{M,s}$, accept, else reject
\end{itemize}
First check that $H_{M,s}$ accepts either the empty language or
$L_X$. This is used in the following:
$A$ accepts $\langle M,s \rangle$ iff $B$ accepts $H_{M,s}$, iff
%
$H_{M,s}$ has property $P$, iff
%
$H_{M,s}$ accepts $L_X$, iff
%
$M$ accepts $s$.
So, $A$ is a decider for $A_{TM}$, clearly impossible, so $B$ does not
exist, and $Pos_P$ is not decidable.
\end{proof}

Rice also has a second theorem: every non-monotone property is not
recognizable; it is worth having a look at it!

\begin{exercise}
In the context of Rice's theorem:
\begin{itemize}
\item
What can you conclude regarding whether $Pos_P$ and/or $Neg_P$ are
recognizable?

\item Use Rice for new proofs of old theorems.
\end{itemize}
\end{exercise}


%###############################################################################
\chapter{Reductions}

This chapter introduces relations between languages (or problems)
that are known as \emph{reductions}. They allow transferring
what is known about decidability or recognizability from one to
the other.

A reduction maps one problem $L_1$ to another problem $L_2$. 
It may be a bit counterintuitive at first, but for such
a mapping to be possible $L_2$ must be in some sense {\em more difficult}
than $L_1$. This is because a method to solve $L_2$ is (together with the reduction) powerful
enough to solve $L_1$. We will make all this very precise. It is important
to get a good grasp on the reductions in this section, because in complexity
theory, we use similar reductions, so it is a recurring concept.

%===============================================================================
\section{Many-one Reduction}\label{mappingreduction}

We start with a highly structured class of reductions, the many-one reductions.
These reductions are defined in terms of a Turing computable function.

\begin{definition}[Turing-computable function]
A function $f$ is Turing-computable if there exists a Turing machine that on input $s$
halts (in $q_a$/$q_e$) with $f(s)$ on the tape.
\end{definition}

\begin{definition}[Many-one Reduction of languages] \label{manyone}
A many-one reduction of language $L_1$ (over $\Sigma_1$) to language
$L_2$ (over $\Sigma_2$) is a total computable function $f$ from
$\Sigma_1^*$ to $\Sigma_2^*$ so that: 
\begin{equation*}
\forall s \in \Sigma_1^*: \quad s \in L_1 ~~\Leftrightarrow~~ f(s) \in L_2
\end{equation*}

We denote this by $L_1 \leq_m L_2$, and we can also say {\em $L_1$
  reduces to $L_2$}, or {\em $L_1$ is mapping-reducible to $L_2$}.
\end{definition}

Figure~\ref{mapred} illustrates the condition on $f$. Observe that
the condition can be equivallently written as:
\begin{equation*}
\forall s \in \Sigma_1^*: \quad s \not\in L_1 ~~\Leftrightarrow~~ f(s) \not\in L_2
\end{equation*}
or as
\begin{equation*}
\forall s \in \Sigma_1^*: \quad s \in \bar{L}_1 ~~\Leftrightarrow~~ f(s) \in \bar{L}_2
\end{equation*}
Sometimes it is also written as:
\begin{equation*}
f(L_1) \subseteq L_2 ~~\wedge~~ f(\bar{L}_1) \subseteq \bar{L}_2
\end{equation*}

\begin{figure}[th]
	\centering
	\includegraphics[  width=0.6\linewidth,  keepaspectratio]{mapred}
	\caption{Schematic representation of a reduction\label{mapred}}
\end{figure}



A reduction $f$ offers a method to transform the decision question
about $L_1$ into a decision question about $L_2$. Indeed,
$s \in L_1?$ can be answered by deciding about $f(s) \in L_2$. The
next theorems make that more precise. 

\begin{theorem}
If $L_1 \leq_m L_2$ and $L_2$ is decidable, then $L_1$ is decidable.
\end{theorem}

\begin{theorem}
If $L_1 \leq_m L_2$ and $L_2$ is recognizable, then $L_1$ is recognizable.
\end{theorem}

\begin{corollary}
If $L_1 \leq_m L_2$ and $L_1$ is not recognizable, then $L_2$ is not
recognizable.  If $L_1 \leq_m L_2$ and $L_1$ is not decidable, then
$L_2$ is not decidable.
\end{corollary}
You must be able to prove the above.

We have used reductions informally in previous theorems, e.g. on page
\pageref{reduction1} when we proved that $EQ_{TM}$ is not decidable.
Let's do it now based on the definitions.

\begin{theorem}
$EQ_{TM}$ is not decidable.
\end{theorem}
\begin{proof}
We reduce $E_{TM}$ to $EQ_{TM}$: $f$ maps input $\langle M \rangle$ to
$\langle M,M_{\emptyset} \rangle$; $M_{\emptyset}$ a Turing machine accepting the
empty language. Clearly, $f$ is Turing computable.

So, $E_{TM} \leq_m EQ_{TM}$, and since we know already that $E_{TM}$
is not decidable, $EQ_{TM}$ is not decidable either.
\end{proof}

\begin{theorem}
$A \leq_m B$ if and only if  $\overline{A} \leq_m \overline{B}$.
\end{theorem}
\begin{proof}
Do it yourself.
\end{proof}


Also on page \pageref{nonreductie1}, we used a reduction from
$\overline{A}_{TM}$ to $E_{TM}$: we mapped $\langle M,s \rangle$ to
$\langle M_s \rangle$ ...  Note that a mapping reduction from
$A_{TM}$ to $E_{TM}$ does not exist - can you explain why? Hint, 
what do you know about the co-recognizability of both?


\begin{theorem}
$EQ_{TM}$ is not recognizable and not co-recognizable.
\end{theorem}
\begin{proof}
We construct two mapping reductions: $A_{TM} \leq_m
\overline{EQ}_{TM}$ and $A_{TM} \leq_m EQ_{TM}$. Since
$\overline{A}_{TM}$ is not recognizable, the result follows.
\begin{description}
\item[$A_{TM} \leq_m \overline{EQ}_{TM}$:] ~\\ 
$f$ maps $\langle M,s \rangle$ to $\langle M_s,M_{\emptyset} \rangle$;
  $M_s$ is a machine that accepts every string if $M$ accepts $s$; clearly
  $f$ is computable; we should also prove the other conditions:
\begin{itemize}
\item if $M$ accepts $s$, then the languages of $M_s$ and $M_{\emptyset}$ are
  different
\item if $M$ does not accept $s$, then the languages of $M_s$ and $M_{\emptyset}$
  are equal
\end{itemize}

\item[$A_{TM} \leq_m EQ_{TM}$:] ~\\
$f$ maps $\langle M,s \rangle$ to $\langle M_s,M_{\Sigma^*}
  \rangle$; $M_{\Sigma^*}$ is a machine accepting all strings; $M_s$
  is like before; checking the conditions on $f$ should be
straightforward.
\end{description}
\end{proof}

%===============================================================================
\section{Oracle Machines and a Decidability Hierarchy}

Suppose we had another way to decide $A_{TM}$ - not via a TM of course -
would there be a way to use that power to decide everything? Let's
make the question a bit more concrete:
\begin{itemize}
\item Since no TM can decide $A_{TM}$, we must name this device
  differently: it is an {\em oracle}; we get back to the
  implementation of an oracle later ...

\item A TM must be able to consult the oracle, i.e. the oracle for
  $A_{TM}$ can be called by a TM as a subroutine, with a string $s$ as
  input to the oracle; the oracle needs only a finite number of steps
  to give its decision (is $s \in A_{TM}$?) to the TM.

\item In this way, we build a so called oracle machine $O^{A_{TM}}$:
  it is a TM that can ask questions to the oracle for $A_{TM}$.
\end{itemize}

It is clear that we can make an $O^{A_{TM}}$ that decides $A_{TM}$:
give the input $\langle M,s \rangle$ to the oracle, and return its
answer. So, the set of oracle machines with oracle $A_{TM}$ is
strictly stronger than the set of Turing machines. Here comes another
example:

\begin{theorem}
There exists an $O^{A_{TM}}$ that decides $E_{TM}$.
\end{theorem}
\begin{proof}
We construct $O^{A_{TM}}$ as follows: on input $\langle M \rangle$
$O^{A_{TM}}$ performs the following actions
\begin{itemize}
\item
it constructs a Turing machine $P$ that on input $w$ performs the
following actions
\begin{itemize}
\item run $M$ on all strings of $\Sigma^*$ (*)\label{allestrings}
\item if $M$ accepts a string, accept
\end{itemize}

\item ask the oracle for $A_{TM}$ whether $\langle P,x \rangle \in
  A_{TM}$

\item if the oracle answers {\bf yes}, reject; otherwise accept
\end{itemize}
If $L_M \neq \emptyset$, then $P$ accepts every input, and for sure
also input $x$; so, the oracle answers {\bf yes} and $O^{A_{TM}}$ must
reject. Vice versa: if $L_M = \emptyset$, then $O^{A_{TM}}$
accepts. We conclude that $O^{A_{TM}}$ decides the language $E_{TM}$.
\end{proof}

The theorem proves that $E_{TM}$ is decidable relative to
$A_{TM}$. The relevant definition is:

\begin{definition}[Turing-reducible]
A language $A$ is Turing-reducible to language $B$, if $A$
is decidable relative to $B$, i.e. there exists an oracle machine
$O^B$ that decides $A$. We write this as $A \leq_T B$.
\end{definition}

The definition supports our intuition about what it means for one
language to be reducible to another:

\begin{theorem}
If $A \leq_T B$ and $B$ is decidable, then $A$ is decidable.
\end{theorem}

So how does Turing-reducibility differ from mapping-reducibility? Turing-reducibility is clearly weaker: it just requires decidability of one language to be linked to another, without there being a mapping $f$ between them. But how can there be a link without a mapping? Here's an example that should clear that up.

\begin{example}
There cannot be a mapping reduction $f$ as defined in Definition~\ref{manyone} between $H_{TM}$ (the halting problem) and $\overline{H_{TM}}$ (the looping problem). Let's say we have a tuple $\langle M,s\rangle$, and (but we haven't checked this yet) $\langle M,s\rangle\in H_{TM}$. Suppose we had a mapping reduction $f$. Since $\langle M,s\rangle$ is in $H_{TM}$, $f$ should map it to a tuple $\langle M',s'\rangle \in \overline{H_{TM}}$. But which one? Clearly not $M = M'$ and $s = s'$, because $\langle M,s\rangle$ can only be in one of the two languages. Since $f$ has no more information than $\langle M,s\rangle$, the only alternative is to map to an arbitrarily different $M'$, or $s'$, or both, and in all cases, there is no way for $f$ to ensure that $\langle M',s'\rangle \in \overline{H_{TM}}$. So $H_{TM} \not\leq_m \overline{H_{TM}}$.

Yet, a decider that can say whether $\langle M,s\rangle \in H_{TM}$ immediately tells you whether $\langle M,s\rangle \in \overline{H_{TM}}$: it's just the opposite answer! Hence, there is no mapping between the halting problem and looping problem, yet their decidability is linked. This makes each one strictly Turing-reducible to the other: $H_{TM} \leq_T \overline{H_{TM}}$ and $\overline{H_{TM}} \leq_T H_{TM}$.
\end{example}

You might have already suspected the following to be true.
\begin{theorem}
If $A \leq_m B$ then also $A \leq_T B$.

In other words: $\leq_m$ is finer than $\leq_T$.
\end{theorem}

Time for a description of an oracle for any language $L$: each
string has a running number in the lexicographic order on strings
(shorter before longer, and alphabetic). That means that the language
can be represented as an infinite bitmap: bit $i$ is one, if the
$i^{th}$ string is in L, and otherwise zero. That bitmap could be on a
tape. When an oracle gets a membership question about $s$, it first
computes its corresponding number, and then looks up the right bit
... So, do oracles exist or not?

Another question: can the class of oracle machines with an oracle for
$A_{TM}$ decide all languages? Certainly not, as a cardinality
argument can show you! So there exists a language $X$ that is not
decidable with an oracle for $A_{TM}$. You can now repeat the argument
for the $X$ oracle ... and you soon note that there is an infinite
hierarchy of ever more difficult languages. As a sidenote: also
complexity theory has its oracle machines and a nice hierarchy. The
difference is: the particular complexity hierarchy would still
collapse in case it turns out that $\P = \NP$. Our computability
hierarchy however stands strong!

\begin{exercise}
In the proof on page \pageref{allestrings},
a statement is marked by a (*): how can we let $M$ run on all strings?
There are infinitely many of them ...
\end{exercise}


%===============================================================================
\section{The Computable and the Recursive Functions}

We gave a definition of Turing computable function in the previous
section. It was rather abstract and {\em existential}. We can also
get a grip on which functions are Turing-computable by
working {\em bottom-up}: we start with very simple functions, and
compose them with simple operators. That is what Kurt Gödel and
Jacques Herbrand did.

%-------------------------------------------------------------------------------
\subsection{The Primitive Recursive Functions}

The first attempt of Gödel and Herbrand is known as the \emph{primitive
recursive functions}. These are all the functions on natural numbers that can
constructed out of the first set of building blocks they devised. It was discovered
that this first attempt fell short: it does not capture all Turing-computable
functions; more on that later.

There are five building blocks for primitive recursive functions: three basic
functions (or rather families of functions) and two operators (or higher-order
functions)
for combining functions.

%- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
\subsubsection{The Basic Functions}
There are three basic functions.
\begin{itemize}
\item
The null function: $\mathit{null} : \N \rightarrow \N$
\begin{equation*}
\mathit{null}(x) = 0
\end{equation*}

\item
The successor function: $\mathit{succ} : \N \rightarrow \N$
\begin{equation*}
\mathit{succ}(x) = x + 1
\end{equation*}

\item
The family of projection functions : $p_i^n : \N^n \rightarrow \N$
\begin{equation*}
p_i^n(x_1,x_2,...,x_n) = x_i
\end{equation*}
There is a projection function $p_i^n$ for every $n \geq 1$ and every $1 \leq
i \leq n$. Notably, $p_1^1(x) = x$ is the identity function.
\end{itemize}

%- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
\subsubsection{The Operators}

The two families of operators are higher-order functions that combine the basic
functions to create more complex behavior.

\begin{itemize}
\item The family of function composition operators $\mathit{Cn}[f,g_1,\ldots,g_m] : \N^n \to \N$
where $f : \N^m \to \N$ and $g_i : \N^n \to \N$ (for $i \in 1,\ldots,m$).
\begin{equation*}
\mathit{Cn}[f,g_1,\ldots,g_m](x_1,\ldots,x_n) = f(g_1(x_1,\ldots,x_n),\ldots,g_m(x_1,\ldots,x_n))
\end{equation*}
This operator takes $1 + m$ function parameters $f$ and $g_1,\ldots,g_m$, and
$n$ natural numbers $x_1,\ldots,x_n$. There is one such operator for every $m
\geq 1$ and $n \geq 1$. For example, we get regular function composition when
$m = n = 1$. For instance, $\mathit{Cn}[\mathit{succ},\mathit{succ}]$ 
denotes $\mathit{succ}\,\circ\,\mathit{succ}$, which is the function that
adds $2$ to its input.
Similarly, $\mathit{Cn}[\mathit{succ},\mathit{null}]$ 
denotes $\mathit{succ}\,\circ\,\mathit{null}$, which is the function that
returns $1$ regardless of its input.

By repeated nesting we get more complex composition chains, e.g., 
$\mathit{Cn}[\mathit{succ},\mathit{Cn}[\mathit{succ},\mathit{succ}]]$
adds $3$ to its input.

\item The family of primitive recursion operators $\mathit{Pr}[z,s] : \N^k \times \N \to \N$
where $z : \N^k \to \N$ and $s : \N^k \times \N \times \N \to \N$.
\begin{eqnarray*}
\mathit{Pr}[z,s](x_1,\ldots,x_k,0) & = & z(x_1,\ldots,x_k) \\
\mathit{Pr}[z,s](x_1,\ldots,x_k,y' + 1) & = & s(x_1,\ldots,x_k,y',\mathit{Pr}[z,s](x_1,\ldots,x_k,y'))
\end{eqnarray*}
The operator takes two function parameters $z$ and $s$, where $z$ handles the base case (zero) and 
$s$ handles the recursive case (successor). It also takes $k + 1$ natural numbers: 
the $x_1,\ldots,x_n$ are the auxiliary parameters and the $y$ parameter is the main one, the one on which
the operator performs structural recursion.

The base case is if $y$ is $0$. Then the operator returns the result of $z$ on the auxiliary parameters.
The recursive case is when $y$ is of the form $y' + 1$. Then the operator recurses on  $y$'.
The $s$ function is used to combine the auxiliary parameters, the value of $y'$ and the recursive
result in the overall result.

The recursion operator allows us to define addition of natural numbers, which satisfies
the following two equations:
\begin{eqnarray*}
\mathit{add}(x,0) & = & x \\
\mathit{add}(x,y'+1) & = & 1 + \mathit{add}(x,y') 
\end{eqnarray*}
Hence, we can take $z = p_1^1$ for the base case, returning $x$ given $x$.
For the recursive case, we take $s = \mathit{Cp}[\mathit{succ},p_3^3]$ 
to return $1 + \mathit{add}(x,y')$ given $x$, $y'$ and $\mathit{add}(x,y')$. In short,
the $\mathit{add}$ function can be constructed as:
\begin{equation*}
\mathit{add} = \mathit{Pr}[p_1^1,\mathit{Cp}[\mathit{succ},p_3^3]]
\end{equation*}
\end{itemize}


\begin{definition}[Primitive recursive function]
All functions that can be constructed from the basic functions and by using composition and primitive recursion, are named
{\bf primitive recursive}.
\end{definition}


Primitive recursive functions are total. They can be
computed by FOR-programs.

\begin{example}
More examples of primitive recursive functions:
\begin{itemize}
\item
$Cn[succ,null]$ is the constant function 1

\item
$Cn[succ,Cn[succ,Cn[succ,null]]]$ is the constant function 3

\item

$Pr[null,Cn[sum,p_1^3, p_3^3]]$ = product of 2 inputs ($\N^2 \rightarrow \N$)

\item
factorial, minus1 (or predecessor), ...

\end{itemize}
\end{example}

%-------------------------------------------------------------------------------
\subsection{The Recursive Functions}

\subsubsection{A Non-Primitive Recursive Function: the Ackermann Function}

Initially, Gödel stopped at the primitive recursive functions when he
was trying to define all {\em computable functions}: he was doing that
without referring to TMs. However, in 1928, Wilhelm Ackermann, a
student of David Hilbert, published what we now know as the Ackermann
function: this function is clearly {\em computable} in an intuitive
sense and understanding of computability (and also by a TM, but they
did not exist in 1928!), still, the Ackermann function is not
primitive recursive.\footnote{Actually, another student of Hilbert,
 Gabriel Sudan, was the first to establish a computable, but not
 primitive recursive function.} Here follows the definition of
Ackermann's function with two arguments:
\begin{equation*}
\left\{\begin{aligned}
	\text{Ack}(0, y) &= y + 1\\
	\text{Ack}(x + 1, 0) &= \text{Ack}(x, 1) \\
	\text{Ack}(x + 1, y + 1) &= \text{Ack}(x, \text{Ack}(x + 1, y ))
\end{aligned}\right.
\end{equation*}
This function is total (try to understand this!) and monotonically increasing. 
In fact, it increases faster than any primitive recursive function can: this
was the argument used to prove that it is not primitive recursive.

Often one uses the words {\em Ackermann's function} to mean a function
with one argument only. It is defined as
\verb|Ack(n) = Ack(n,n)|. This function also increases faster than any
primitive recursive function. Its inverse increases so slowly that it appears
to be near-contant; it is important in complexity analysis of algorithms, e.g.,
for the Union-Find algorithm.

\subsubsection{Unbounded Minimization}

Gödel had to enlarge his class of functions to accommodate the Ackermann
function (and many other examples of non-primitive recursive functions).
To do that he introduced one more operator, {\em unbounded minimization}.

The family of primitive recursion operators $\mathit{Mn}[f] : \N^k \to N$
where $f : \N \times \N^k \to N$.
\begin{equation*}
\mathit{Mn}[f](x_1,\ldots,x_k) = y
\end{equation*}
provided that $f(y,x_1,\ldots,x_k) = 0$ and $\forall z < y, f(z,x_1,\ldots,x_n,z) \neq 0$.
Loosely speaking, $\mathit{Mn}[f]$ gives the minimal roots of f.

Where primitive recursive functions correspond to imperative programs with FOR-loops,
the minimization operator introduces WHILE-loops. Specifically, the operator
itself can be implemented as follows.
\newpage
\begin{verbatim}
    y = 0;
    while (f(y,x) != 0)
       y++;
    return y;
\end{verbatim}
This imperative implementation can end up in an infinite loop in two
ways\ldots{} Such an infinite loop corresponds to the function being undefined,
or partial,
for that input.

\begin{definition}[Recursive functions]
{\bf Recursive functions} are constructed from the basic functions, and by
applying $\mathit{Pr}$, $\mathit{Cn}$ and $\mathit{Mn}$. 
\end{definition}
One also names these functions {\em $\mu$-recursive} because $\mu$ is the
original name for the minimalization operator $\mathit{Mn}$.

These functions can be computed by a Turing machine, and
vice versa. It means that the Turing-computable functions coincide with
the recursive functions. It is clear from the definition of unbounded
minimization that recursive functions can be partial. That is
consistent with the notion of Turing-computable: a TM often only defines
a partial function. Yet, the Ackermann function shows that really
recursive functions can also be total.

\paragraph{Note:}
We have not shown how to write the Ackermann function with the minimization
operator. The reason for that is that, while it is possible, it is rather
laborious and lengthy. It involves:
\begin{itemize}
\item encoding the tuple of parameters $(x,y)$, which represents the call,
      Ack($x$,$y$) as a single number;
\item encoding nested calls as nested tuples and those nested tuples as a single
      number;
\item writing a step function that simulates a single evaluation step on the
      encoding of nested tuples as a single number;
\item writing a function that iterates a step function a given number of times with primitive
      recursion; and
\item finding the smallest iteration number that runs the simulation to completion with
      the minimization operator.
\end{itemize}

\begin{exercise}
We can link recursive functions with computability.
\begin{itemize}
\item
Understand and/or prove: if the domain of a partial recursive function
is decidable, then it can be extended to a total function that is also
recursive.
\item
Do partial recursive functions exist whose domain is not decidable?
\item
Is the domain of a (partial) recursive function recognizable?
\item
Is the range of a (partial) recursive function recognizable?
\end{itemize}
\end{exercise}


%===============================================================================
\section{The {\em Post Correspondence Problem}}

\begingroup
\begin{wrapfigure}[9]{r}{.2\textwidth}
\includegraphics[width=.2\textwidth,keepaspectratio]{static/post}
\end{wrapfigure}

Emil Post was one of the founding fathers of recursion theory, and he
made contributions to universal computing mechanisms. The word {\em
  correspondence} is related to {\em similarity, agreement}, not to
sending mail by post. Emil Post developed the following game:

You have a finite collection of dominoes, each with two strings
(instead of the usual numbers). Of each type, there is an unlimited
supply for this game. If you put some dominoes next to each other, you
see an upper string and a lower string. The question is: is it
possible to lay out a finite number of dominoes so that the upper and
lower strings are the same?

\endgroup

\begin{figure}[h]
	\centering
	\includegraphics[%
	  width=0.5\linewidth,
	  keepaspectratio]{pcp1}
	\caption{Four given dominoes and a solution\label{pcp1}}
\end{figure}

Figure~\ref{pcp1} shows that you don't need to use all dominoes and
that one type of domino can be used more than once.

We are dealing here with a decision problem: it is enough to answer
yes or no to the question, there is no need to construct the
corresponding sequence of dominoes.

This simple game is (1) undecidable and (2) powerful enough to mimic all
computations by all Turing machines. Actually, the former follows from
the latter, as soon as we have shown that the halting of a TM can be
reduced to the existence of a solution to the PCP.

\subsection{Turning a Turing machine into a PCP game}
Any Turing machine can be turned into a set of PCP dominoes, as can the TM's input; it then remains to be proved that playing the PCP game with the input domino is equivalent to running the TM on its input. The following algorithm is after the books by Sipser and Minsky.

\begin{code}[TM to PCP dominoes]
Starting from a TM $(\Sigma,\Gamma,\delta,Q,q_0,q_a,q_r)$:
\begin{enumerate}
\item Add the three fixed dominoes: one with bottom $\$$ and top $q_a \$\$$, and two with top $\$$ where one has bottom $\#\$$ and the other has bottom $\$\#$.

\item For each $\sigma\in \Gamma \cup \{\$\}$, add three dominoes: one with $\sigma$ as top and bottom string, and two with bottom $q_a$ where one has top $q_a \sigma$ and the other has top $\sigma q_a$.

\item For each $\delta(q,\sigma) = (p,\tau,D)$ where $p\neq q_r$:
\begin{itemize}
	\item If $D=L$: add a domino with top $\sigma q$ and bottom $p\tau$.   % This simulates a movement L that writes nothing and then an S that writes tau. Although this isn't exactly a simulation of just L, I'm pretty sure a Turing machine that has this restriction is still equivalent to one that can make single moves leftward.
	\item If $D=S$: add a domino with top $q \sigma$ and bottom $p\tau$.
	\item If $D=R$: add a domino with top $q \sigma$ and bottom $\tau p$.
\end{itemize}

\item Every TM input string $s$ corresponds to a starting domino with $\$$ on top and $\$q_0 s\$$ on the bottom.
\end{enumerate}
\end{code}

\begin{example}
In this example we use X for the start state, A for the accepting
state, Z for the reject state. The alphabet is $\{a,b\}$, and the
transition table looks like:

\begin{center}
\begin{tabular}{|r||l|l||l|l|l|}
\hline
1 & X & a & B & a & R \\
2 & X & b & Z & \_ & \_ \\
3 & X & \# & A & \# & S \\
4 & B & a  & Z & \_ & \_ \\
5 & B & b  & X & b  & R  \\
6 & B & \# & Z & \_ & \_ \\
\hline
\end{tabular}
\end{center}

The numbers at the left are there for ease of reference. Check that
this TM accepts the language $(ab)^*$. Take $ab$ as the input string.
%
We introduce an extra symbol \$. We now define the dominoes needed in
the Post game for imitating the above Turing machine:

\begin{enumerate}
\item The fixed dominoes:

the end domino
\begin{tabular}{|l|}
\hline
A\$\$                     \\ \hline
   \$                     \\
\hline
\end{tabular}
and the blank generators left and right
\begin{tabular}{|l||l|}
\hline
        \$   & \$         \\ \hline
        \$\# & \#\$       \\
\hline
\end{tabular}


\item We have the 4 dominoes:

\begin{tabular}{|l||l||l||l|}
\hline
a & b & \$ & \# \\ \hline
a & b & \$ & \# \\ \hline
\end{tabular}

and 8 dominoes with Ax or xA as the upper string, and A as lower
string:

\begin{tabular}{|l||l||l||l|}
\hline
aA & bA & \$A & \#A \\ \hline
A & A & A & A \\
\hline
\end{tabular}\quad
\begin{tabular}{|l||l||l||l|}
\hline
Aa & Ab & A\$ & A\# \\ \hline
A & A & A & A \\
\hline
\end{tabular}

\item Now for the transitions to non-rejecting states:
\begin{itemize}
\item Row 1:
\begin{tabular}{|l|}
\hline
Xa \\ \hline
aB \\
\hline
\end{tabular}

\item Row 3:
\begin{tabular}{|l|}
\hline
X\# \\ \hline
A\# \\
\hline
\end{tabular}

\item Row 5:
\begin{tabular}{|l|}
\hline
Bb \\ \hline
bX \\
\hline
\end{tabular}

\end{itemize}

\item Finally, the input $ab$ turns into a starting domino
\begin{tabular}{|l|}
\hline
\$ \\ \hline
\$Xab\$ \\
\hline
\end{tabular}
\end{enumerate}
\end{example}

For this example, the question is now: construct a correspondence with these dominoes,
starting from the input domino. (The general PCP allows to
  start from an arbitrary domino: one problem can be reduced to the
  other.) Here is the solution:


% \hspace{-1.5cm}
{\footnotesize
\begin{tabular}{|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|}
\hline
\$      & Xa & b & \$   & a & Bb & \# & \$ & a & b & X\# & \$ &  a & bA & \# & \$ & aA & \# & \$ & A\# & \$ & A\$\$\\ \hline
\$Xab\$ & aB & b & \#\$ & a & bX & \# & \$ & a & b & A\# & \$ &  a &  A & \# & \$ & A  & \# & \$ & A   & \$ & \$  \\
\hline
\end{tabular}
}


In the lower half, you see a configuration between any two occurrences of \$;
two subsequent configurations are linked by the
transition function until the accepting state appears. After that,
the configuration is emptied until only the accepting state and the
end domino appears. Now, upper and lower are equal.


\begin{exercise}
With respect to the above example and algorithm:
\begin{itemize}
\item
Convince yourself that by starting with an input string not in the
language $(ab)^*$, no correspondence can be found.

\item Find out about the general correspondence between $\delta$ and the
dominoes. The example did not have movements to the left; can you justify the algorithm's domino for $D = L$? How would you use it in a PCP solution?

\item Formulate more precisely the link between PCP and $H_{TM}$.
\end{itemize}
\end{exercise}




\paragraph{An extra: the Post tag machine}

E. Post is also known for his {\em tag systems} or {\em Post tag
  machines}. Just a small example of a 2-tag system:

\begin{itemize}
\item the alphabet is $\{a,b,c\}$ and there is also a halt symbol H
\item the rules are
\begin{itemize}
\item a \rpijl aa
\item b \rpijl accH
\item c \rpijl a
\end{itemize}

\item the initial word is aab
\item and here is a derivation starting from the initial word

aab \rpijl baa \rpijl aaccH \rpijl ccHaa \rpijl Haaa
\end{itemize}

The general rule for rewriting is: the first letter of the word
decides which rule is used (this could be the source of
non-determinism). Now add the right side of such a rule at the end of
the string, and erase 2 symbols at the front (the same 2 as in 2-tag
system).

The derivation ends when H appears at the front of the string: the
rest of the string can be considered the result of a computation
starting with the initial string.

This rewriting system is also Turing complete.


2-tag systems have been used to simulate small UTMs.




\section{The Busy Beaver and Fast Increasing Functions}

The Ackermann function showed us that the fact that a function
increases fast, can be an indication that extra machinery is needed to
compute its values. For the Ackermann function, that worked with
Turing machines: TMs can implement unbounded minimization. The essence
is to {\em find the {\bf smallest} number with a particular property
  V}. This can be implemented as
\begin{verbatim}
    i = 0;
    while not V(i)
       i++;
\end{verbatim}
and you already know that it does not finish if no i satisfies V.

We introduce another novelty here, just for trying it out: {\em
  unbounded maximization}, i.e. {\em find the {\bf largest} number
  with a particular property V}. How could we implement that? Here is
a first attempt:

\begin{verbatim}
    i = +infinity;
    while not V(i)
        i--;
\end{verbatim}•

That has no chance of working - please argue. Here is a second attempt:
\begin{verbatim}
    i = 0;
    m = -1;
    while (i < +infinity) {
        if V(i)
            m = i;
        i++;
    }
\end{verbatim}
and you can certainly spot the weakness here as well ...

This shows that the introduction of a method to construct new
functions from old ones has its pitfalls.


\subsection{The busy beaver}

In 1962, Tibor Rad\'{o} invented the following function $S$:
consider the class of all TMs with alphabet $\{0,1\}$ and $n$
states. There are only a finite number of them. We can let these
TMs run with an initial empty tape - say all zeros. We then count for
each machine how many transitions it takes until it stops (in $q_a$ or
$q_r$).  A busy beaver is a champion in its class, i.e. for a given
$n$, no other machine in its class needs more transitions before it
stops. We define $S(n)$ as the number of transitions the busy beaver
in class $n$ makes. Tibor Rad\'{o} restricted the TMs to make a head
movement at each transition, and in terms of how often the head moves,
but that does not matter.

Clearly, $S$ is defined as a total function with signature $\N
\rightarrow \N$. The question {\em is $S$ Turing computable} is
relevant.


It feels possible to construct a busy beaver for a small $n$: exhaustively construct
all TMs with $n$ states, start them with an empty tape
and start counting.

There is however a problem: because of the halting problem, we know we
do not always know in advance whether a machine eventually stops on
the empty input. That means that when a machine runs very long, and we
are losing patience, we better come up with a proof that the machine
is actually in a loop, or we must accept that we can only
construct lower bounds for $S(n)$. This is really a problem, even for
a small $n$, because there exist very small universal TMs!

Nevertheless, the values for $S(n)$ are known exactly for $n <
5$. For $n = 5$ the current record is 47 176 870, but it is not known
about some machines whether they are in a loop, so that record could
still be broken. The record for $n = 6$ is $10^{2879}$.

$S$ is obtained by {\em bounded} maximization, but over a
non-computable set: the set of halting TMs with $n$ states. That is
the inherent reason why $S$ is not computable, even though the
function is total and in the abstract sense well-defined.

Lots of open problems in mathematics could be solved {\em easily} if
we knew $S$ for some $n$.

\begin{exercise}
Which sentences below are true? Please give good arguments for your answer.
\begin{itemize}
\item $S(n)$ can be determined for every $n$, but it is $\infty$ for
  large enough $n$s.

\item It is possible that for a large enough $n$, $Ack(n) > S(n)$.

\item Every function that increases slower than a given primitive recursive
function is primitive recursive.
\end{itemize}
\end{exercise}

